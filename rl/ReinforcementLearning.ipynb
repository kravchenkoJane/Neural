{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с учителем\n",
    "\n",
    "Если у нас есть набор примеров с правильными ответами, то мы используем эту выборку для обучения нашей модели, а после обучения, применяем её к неразмеченным данным. Именно этот подход мы использовали, когда обучали классификатор для MNIST, подавая на вход сети картинки с изображениями рукописных цифр и считая градиент для подстройки весов на основе разницы между известным лэйблом цифры и выходом нейросети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение без учителя\n",
    "\n",
    "В некоторых случаях у нас нет размеченных данных, на которых мы могли бы заранее обучить модель. Но, при решении некоторых задач, можно обойтись без размеченной выборки. Примером такой задачи является задача кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с подкреплением\n",
    "\n",
    "В некоторых случаях существующие методы обучения без учителя нам не подходят. В то же время у нас нет возможности создать качественную обучающую выборку. При этом мы можем постфактум оценить действия нашей модели и использовать эту оценку подстроить модель так, чтобы она чаще совершала желательные действия и реже - нежелательные. В литературе такую оценку называют вознаграждением (reward), а обучение строится таким образом, чтобы это модель стремилась максимизироавть получаемое вознаграждение.\n",
    "\n",
    "### Терминология: Агент и среда\n",
    "\n",
    "Агент и среда - ключевые понятия в обучении с подкрелением.\n",
    "\n",
    "**Агент** - программа, принимающая решение о дальнейших действиях на основе информации о состоянии среды.\n",
    "\n",
    "**Среда** - это мир, в котором агент должен \"выживать\", т.е. всё, с чем агент может прямо или косвенно взаимодействовать. Среда обладает состоянием (State), агент может влиять на среду, совершая какие-то действия (Actions), переводя среду при этом из одного состояния в другое и получая какое-то вознаграждение. Среда описывается пространством возможных состояний. Конкретное состояние - вектор в этом пространстве.\n",
    "\n",
    "<img src=\"agend_and_environment.gif\">\n",
    "\n",
    "В зависимости от конкретной задачи, агент может наблюдать либо полное состояние среды, либо только некоторую его часть. Во втором случае, агенту может потребоваться какое-то внутреннее представление полного состояния, которое будет обновляться по мере получения новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наиболее яркие примеры использования обучения с подкреплением\n",
    "- В 2013 году DeepMind публикует статью Playing Atari with Deep Reinforcement Learning, где нейросети обучаются игре в старые игры от Atari, используя анализ изображения.\n",
    "- В 2016 году нейросеть AlphaGO Google DeepMind  обыгрывает одного из сильнейших игроков в Go - Ли Седоля. При обучении AlphaGo использовались партии игры живых людей. Чуть позже будет представлена AlphaGO Zero, обучение которой было полностью построено на игре с самой собой. Новая сеть выиграла у старой со счётом 100:0, причём аппаратные ресурсы сократились с 48 TPU до 4 TPU (Tensor Processing Unit Google).\n",
    "- В 2017 году нейросеть OpenAI 5 успешно участвует в соревновании по игре Dota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация алгоритмов обучения с подкреплением\n",
    "\n",
    "### Model-free / Model-based\n",
    "\n",
    "Model-free не строит модель окружения или функции вознаграждения. Model-based алгоритм пытаетс предсказывать, каким будет следующее состояние окружения или вознаграждение.\n",
    "\n",
    "### Value-based / policy-based\n",
    "\n",
    "Policy-based  методы оптимизируют напрямую функцию принятия решения агента. Стратегия (policy) обычно представлена распределением вероятности доступных действий. Value-based метод оптимизирует оценку вознаграждения для всех действий и выбирает выбирает то-действие, по которому прогнозируется большее значение. Методы, основанные на Policy Gradients лучше работают при большой размерности пространства действий, а Value-based методы, такие, как Deep Q-Learning требуют меньшего количества повторений для сходимости при малой размерности.\n",
    "\n",
    "### On-Policy / Off-Policy\n",
    "\n",
    "Off-policy подход позволяет учиться на исторических данных или на записанных заранее действиях человека. On-policy - только на собственных действиях агента.\n",
    "\n",
    "### Deterministic Policy / Stochastic Policy\n",
    "\n",
    "В зависимости от среды, наша стратегия может быть либо детерминированной - выбираем сразу определённое действие с помощью argmax, либо стохастической, когда мы окончательное решение принимается с помощью генератора случайных чисел на основе распределения вероятности, выданного сетью.\n",
    " \n",
    "\n",
    "- Q-learning\n",
    "- Deep Q-learning (DQN)\n",
    "- Double Deep Q-learning (DDQN)\n",
    "- Proximal Policy Optimization\n",
    "- Rainbow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Недостатки RL\n",
    "\n",
    "## Низкая скорость обучения (sample efficiency)\n",
    "\n",
    "Общая проблема всех алгоритмов обучения с подкреплением - низкая скорость обучения. В то время, как человеку может быть достаточно одного повторения, чтобы выучить какое-то действие, агенту RL требуется десятки тысяч повторений даже в простых задачах. В какой-то степени это связано с несовершенством архитектуры, но самый большой вклад даёт тот факт, что человек может использовать накопленный в прошлом опыт из других областей. Игра Montezuma's Revenge - популярный подопытная среда для RL в последнее время. И яркий пример низкой эффективности повторений  у алгоритмов RL по сравнению с человеком. \n",
    "\n",
    "Челокек, как правило, быстро понимает, что нужно избегать черепа и забрать ключ, гравитация направлена вниз, а падение с большой высоты опасно. Алгоритму же приходиться обучаться с полного нуля. Если же подменить элементы интерфейса на неочевидные для человека, то его sample-efficency тоже сильно падает (хотя всё-равно лучше, чем RL).\n",
    "\n",
    "<img src=\"game_prior.gif\" width=\"700\">\n",
    "\n",
    "<img src=\"game_no_prior.gif\" width=\"700\">\n",
    "\n",
    "Так же важным фактором являются редкие награды. Часто в ходе одного эпизода алгоритм делает множество различных действий, а награду полуает только в конце. Соответственно, веса сети можно обновить только в конце эпизода и нельзя поощерить или наказать конкретные действия внутри эпизода. В итоге требуется большое количество повторений для достижения оптимальных весов.\n",
    "\n",
    "Один из способов улучшить эффективность при редких наградах - reward shaping - модификация функции награды так, чтобы явно поощерялись какие-то действия внутри эпизода. Но качественно сконструировать такую функцию тяжело, а ошибки в ней могут приводить к неожиданным эффектам:\n",
    "\n",
    "<img src=\"coastrunner.gif\" width=\"700\">\n",
    "\n",
    "В гонке лодок агент получал вознаграждене не только за победу в гонке, но и за сбор всяких ништяков. В итоге он решил, что гонка не очень-то и нужна, достаточно собирать ништяки.\n",
    "\n",
    "<video controls src=\"upsidedown_half_cheetah.mp4\" width=\"700\"> </video>\n",
    "\n",
    "У данного агента мы наблюдаем поподание в локлаьный минимум. Этот агент получает поощерение за набранную скорость. На начальном этапе во время случайного поиска агент обнаружил, что кувыркнуться вперёд даёт хорошее вознаграждение в начале. Постепенно, после нескольких попыток, переворачивание на спину закрепилось, как успешная стратегия. После закрепления такого поведения агент не смог выйти из этого состояния, т.к. оказалось проще научиться двигаться в таком состоянии, чем научиться переворачиваться обратно на ноги.\n",
    "\n",
    "Похожее поведение можно случано получить, если поощерять агента за то, что его ноги оторваны от земли.\n",
    "    \n",
    "<video controls src=\"failed_reacher.mp4\" width=\"700\"> </video>\n",
    "\n",
    "В данном примере случайная инициализация весов получилась такой, что к вращающейся \"конечности\" в каждой точке прикладывалась большая сила. В результате конечность начала быстро вращаться. Сложность избавления от такого поведения заключается в том, что для того, чтобы отступить от такой стратегии, нужно путём исследования случайных действий предпринять несколько попыток, когда робот не будет вращаться, чтобы такие действия могли закрепиться. Это возможно, но в данном запуске этого не произошло."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотека Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример окружения Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\e9\\26\\9b\\8a1a6599a91077a938ac4348cc3d3ac84bfab0dbfddeb4c6e7\\gym-0.15.4-cp36-none-any.whl\n",
      "Requirement already satisfied: scipy in d:\\anaconda3\\envs\\neeeew\\lib\\site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: six in d:\\anaconda3\\envs\\neeeew\\lib\\site-packages (from gym) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in d:\\anaconda3\\envs\\neeeew\\lib\\site-packages (from gym) (1.17.3)\n",
      "Collecting opencv-python\n",
      "  Using cached https://files.pythonhosted.org/packages/fc/3c/f0ac5c2c73df30483e3677b6568ccd19d900a6f78a95bfcf20bbbb468986/opencv_python-4.1.2.30-cp36-cp36m-win_amd64.whl\n",
      "Collecting cloudpickle~=1.2.0\n",
      "  Using cached https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
      "Collecting pyglet<=1.3.2,>=1.2.0\n",
      "  Using cached https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl\n",
      "Processing c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\8b\\99\\a0\\81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\\future-0.18.2-cp36-none-any.whl\n",
      "Installing collected packages: opencv-python, cloudpickle, future, pyglet, gym\n",
      "Successfully installed cloudpickle-1.2.2 future-0.18.2 gym-0.15.4 opencv-python-4.1.2.30 pyglet-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "exception: access violation reading 0x0000000000000000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1c22f504c755>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neeeew\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neeeew\\lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_position\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_position\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neeeew\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     raise ImportError('''\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neeeew\\lib\\site-packages\\pyglet\\gl\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;31m# trickery is for circular import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neeeew\\lib\\site-packages\\pyglet\\window\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1894\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1896\u001b[1;33m     \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neeeew\\lib\\site-packages\\pyglet\\gl\\__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0m_shadow_window\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neeeew\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWin32Window\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_recreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchanges\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neeeew\\lib\\site-packages\\pyglet\\window\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neeeew\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36m_create\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wgl_context\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWin32Canvas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_view_hwnd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wgl_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neeeew\\lib\\site-packages\\pyglet\\gl\\win32.py\u001b[0m in \u001b[0;36mattach\u001b[1;34m(self, canvas)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_pixel_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwglCreateContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhdc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mshare\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext_share\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: exception: access violation reading 0x0000000000000000"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "MAX_NUM_EPISODES = 5000\n",
    "\n",
    "for episode in range(MAX_NUM_EPISODES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0 # To keep track of the total reward obtained in each episode\n",
    "    step = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        \n",
    "        \n",
    "        \n",
    "        action = env.action_space.sample()  # Sample random action.\n",
    "                                            # This will be replaced\n",
    "                                            # by our agent's action\n",
    "                                            # when we # start\n",
    "                                            # developing the agent algorithms\n",
    "        \n",
    "        \n",
    "        next_state, reward, done, info = \\\n",
    "        env.step(action)  # Send the action to the\n",
    "                          # environment and receive       \n",
    "                          # the next_state, reward and\n",
    "                          # whether done or not\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        obs = next_state\n",
    "    print(\"\\n Episode #{} ended in {} steps. total_reward={}\".format(episode, step+1,\n",
    "total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Q-Learning - это метод основанный на представлении функции полезности Q(s, a) в виде таблицы. Соответственно, такой метод применим только для дискретного набора действий и дискретного количества состояний среды, причём желательно, чтобы число возможных действия и число состояний было небольшим. Ограниченно этот метод можно применить к средам с непрерывным состоянием, если его искусственно дискретизировать.\n",
    "\n",
    "\n",
    "\n",
    "Если ввести функцию дисконтированного будущего вознаграждения:\n",
    "\n",
    "\\\\[ R_t = \\sum_{k=0}^{\\infty}{\\gamma^{k} r_{t+k+1}}, \\quad где \\quad \\gamma \\in (0, 1] \\\\],\n",
    "\n",
    "то можно определить функцию \\\\( Q(s, a) \\\\) как математическое ожидание будущего вознаграждения при выполнении действий \\\\( a \\\\) в состоянии \\\\( s \\\\),\n",
    "\n",
    "\\\\[ Q(S, A) = max_{\\pi} \\mathbb{E} [G_t | S_t = s, A_t = a, \\pi] \\\\]\n",
    "\n",
    "SARSA (State, Action, Reward, State, Action) /* on-policy */:\n",
    "\n",
    "\\\\[ Q(S_t, A_t) = (1 - \\alpha) Q(S_t, A_t) + \\alpha [r_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})] \\\\]\n",
    "\n",
    "\\\\[ Q(S_t, A_t) = Q(S_t, A_t) + \\alpha [r_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)] \\\\]\n",
    "\n",
    "\\\\[ TD_{target} = r + \\gamma Q(S_{t+1},A_{t+1})\\\\]\n",
    "\n",
    "\\\\[ TD_{error} = TD_{target} - Q(S_t, A_t) \\\\]\n",
    "\n",
    "Q-обучение /* off-policy */:\n",
    "\n",
    "Уравнение Беллмана:\n",
    "\n",
    "\\\\[ Q(S_t,A_t) = Q(S_t, A_t) + \\alpha [r_{t+1} +  \\gamma \\cdot max_{a}{Q(S_{t+1},a}) - Q(S_t, A_t) ] \\\\]\n",
    "\n",
    "\\\\[ TD_{target} = r + \\gamma max_{a} Q(S_{t+1},a)\\\\]\n",
    "\n",
    "\\\\[ TD_{error} = TD_{target} - Q(S_t, A_t) \\\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\e9\\26\\9b\\8a1a6599a91077a938ac4348cc3d3ac84bfab0dbfddeb4c6e7\\gym-0.15.4-cp36-none-any.whl\n",
      "Collecting opencv-python\n",
      "  Using cached https://files.pythonhosted.org/packages/fc/3c/f0ac5c2c73df30483e3677b6568ccd19d900a6f78a95bfcf20bbbb468986/opencv_python-4.1.2.30-cp36-cp36m-win_amd64.whl\n",
      "Requirement already satisfied: six in d:\\anaconda3\\envs\\neew\\lib\\site-packages (from gym) (1.13.0)\n",
      "Collecting pyglet<=1.3.2,>=1.2.0\n",
      "  Using cached https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.10.4 in d:\\anaconda3\\envs\\neew\\lib\\site-packages (from gym) (1.17.3)\n",
      "Collecting cloudpickle~=1.2.0\n",
      "  Using cached https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy in d:\\anaconda3\\envs\\neew\\lib\\site-packages (from gym) (1.3.1)\n",
      "Processing c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\8b\\99\\a0\\81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\\future-0.18.2-cp36-none-any.whl\n",
      "Installing collected packages: opencv-python, future, pyglet, cloudpickle, gym\n",
      "Successfully installed cloudpickle-1.2.2 future-0.18.2 gym-0.15.4 opencv-python-4.1.2.30 pyglet-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode#:19999 reward:-165.0 best_reward:-112.0 eps:0.034332999948727344"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "EPSILON_MIN = 0.005\n",
    "MAX_NUM_EPISODES = 20000\n",
    "MAX_STEPS_PER_EPISODE = 500\n",
    "max_num_steps = MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE\n",
    "EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps\n",
    "ALPHA = 0.05 # learning rate\n",
    "GAMMA = 0.95 # Discount factor\n",
    "NUM_DISCRETE_BINS = 30  # Number of bins to Discretize each\n",
    "                        # observation dim\n",
    "\n",
    "class QAgent():\n",
    "    def __init__(self, env):\n",
    "        self.obs_shape = env.observation_space.shape\n",
    "        self.obs_high = env.observation_space.high\n",
    "        self.obs_low = env.observation_space.low\n",
    "        self.obs_bins = NUM_DISCRETE_BINS\n",
    "        self.bin_width = (self.obs_high - self.obs_low) \\\n",
    "            / self.obs_bins\n",
    "        self.action_shape = env.action_space.n\n",
    "        # Create a table to represent the Q-values\n",
    "        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,\n",
    "                          self.action_shape)) # (51 x 51 x 3)\n",
    "        self.alpha = ALPHA\n",
    "        self.gamma = GAMMA\n",
    "        self.epsilon = 1.0\n",
    "    \n",
    "    \n",
    "    def discretize(self, obs):\n",
    "        return tuple(((obs - self.obs_low) \\\n",
    "                      / self.bin_width).astype(int))\n",
    "    \n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        discretized_obs  = self.discretize(obs)\n",
    "        # Epsilon-Greedy action selection\n",
    "        if self.epsilon > EPSILON_MIN:\n",
    "            self.epsilon -= EPSILON_DECAY\n",
    "        if np.random.random() > self.epsilon:\n",
    "            return np.argmax(self.Q[discretized_obs])\n",
    "        else:  # choose a random action\n",
    "            return np.random.choice([a for a in range(self.action_shape)])\n",
    "        \n",
    "    \n",
    "    def learn(self, obs, action, reward, next_obs):\n",
    "        discretized_obs = self.discretize(obs)\n",
    "        discretized_next_obs = self.discretize(next_obs)\n",
    "        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])\n",
    "        td_error = td_target - self.Q[discretized_obs][action]\n",
    "        self.Q[discretized_obs][action] +=  self.alpha * td_error\n",
    "\n",
    "\n",
    "def train_Q(agent, env):\n",
    "    best_reward = -float('inf')\n",
    "    for episode in range(MAX_NUM_EPISODES):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        total_reward = 0.0\n",
    "        while not done:\n",
    "            action = agent.get_action(obs)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            agent.learn(obs, action, reward, next_obs)\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "        best_reward = max(best_reward, total_reward)\n",
    "        print(\"\\rEpisode#:{} reward:{} best_reward:{} eps:{}\"\n",
    "             .format(episode, total_reward, best_reward, agent.epsilon), end=\"\")\n",
    "        \n",
    "    # Return the trained policy\n",
    "    return np.argmax(agent.Q, axis=2)\n",
    "\n",
    "\n",
    "def test_Q(agent, env, policy):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    while not done:\n",
    "        action = policy[agent.discretize(obs)]\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    agent = QAgent(env)\n",
    "    learned_policy = train_Q(agent, env)\n",
    "    # Use the Gym Monitor wrapper to evaluate the agent and record video\n",
    "    gym_monitor_path = \"./gym_monitor_output\"\n",
    "    env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)\n",
    "    for _ in range(1000):\n",
    "        test_Q(agent, env, learned_policy)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks\n",
    "\n",
    "Таблицу, аппроксимирующую нашу функцию качества можно заменить нейросетью, которая будет предсказывать значению Q сразу для всех возможных действий. В этом случае нам нужно определить функцию потерь, от которой мы будем считать градиент.\n",
    "\n",
    "\\\\[ L = {TD_{error}}^2 \\\\]\n",
    "\\\\[ L = (r + \\gamma max_a Q(S_t, a) - Q(S_t, A_t))^2 \\\\]\n",
    "\n",
    "# Улучшения DQN\n",
    "\n",
    "## Experience replay\n",
    "\n",
    "В большинстве окружений информация, получаемая агентом распределена не независимо. Т.е. последовательные наблюдения агента сильно коррелированы между собой (что понятно из интуитивных соображений, т.к. большинство окружений, в которых применяется RL, предполагают, что все изменения в них последовательны). Корреляция примеров ухудшает сходимость стохастического градиентного спуска. Таким образом нам нужен способ, который позволяет улучшить распределение примеров для обучения (устранить или снизить корреляцию между ними). Обычно используется метод **проигрывания опыта (experience replay)**. Суть этого метода в том, что мы сохраняем некоторое количество примеров (состояние, действия, вознаграждение) в специальном буфере и для обучения выбираем случайные мини-батчи из этого буфера.\n",
    "\n",
    "Так же **experience replay** позволяет агенту эффективнее использовать свой прошлый опыт.\n",
    "\n",
    "## Double DQN\n",
    "\n",
    "Одной из проблем Q-Networks является неустойчивость. Часто разность ожидаемых вознаграждений для различных действий близка и поскольку выбор действия производится с помощью argmax, то выброс в данных может привести к тому, что выбираемое действие изменится. Для того, чтобы повысить стабильность используется техника **Target Q-Network**. Суть в том, что мы замораживаем веса нашей сети на фиксированное число шагов и затем используем её для вычисления функции ошибки и обучения второй сети. Периодически копируем из веса рабочей сети в Target Q-Network.\n",
    "В следующем примере мы будем делать это каждый эпизод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 25.0   memory length: 26   epsilon: 0.9743224148844496\n",
      "episode: 1   score: 37.0   memory length: 64   epsilon: 0.9379749638258457\n",
      "episode: 2   score: 36.0   memory length: 101   epsilon: 0.9038873549665959\n",
      "episode: 3   score: 23.0   memory length: 125   epsilon: 0.8824417114557717\n",
      "episode: 4   score: 21.0   memory length: 147   epsilon: 0.8632304853107438\n",
      "episode: 5   score: 10.0   memory length: 158   epsilon: 0.8537822855004553\n",
      "episode: 6   score: 53.0   memory length: 212   epsilon: 0.8088788946494789\n",
      "episode: 7   score: 16.0   memory length: 229   epsilon: 0.7952374128525983\n",
      "episode: 8   score: 41.0   memory length: 271   epsilon: 0.7625130999383466\n",
      "episode: 9   score: 20.0   memory length: 292   epsilon: 0.7466594429963713\n",
      "episode: 10   score: 28.0   memory length: 321   epsilon: 0.7253067522353204\n",
      "episode: 11   score: 54.0   memory length: 376   epsilon: 0.686473177834008\n",
      "episode: 12   score: 142.0   memory length: 519   epsilon: 0.5949608504704863\n",
      "episode: 13   score: 178.0   memory length: 698   epsilon: 0.4974057274803321\n",
      "episode: 14   score: 75.0   memory length: 774   epsilon: 0.46098615996978987\n",
      "episode: 15   score: 18.0   memory length: 793   epsilon: 0.4523058066495642\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "episode: 16   score: 500.0   memory length: 1293   epsilon: 0.2742687177907501\n",
      "episode: 17   score: 479.0   memory length: 1773   epsilon: 0.1696721740777052\n",
      "episode: 18   score: 500.0   memory length: 2000   epsilon: 0.10288563388954207\n",
      "episode: 19   score: 500.0   memory length: 2000   epsilon: 0.062387682119314766\n",
      "episode: 20   score: 500.0   memory length: 2000   epsilon: 0.03783057685584513\n",
      "episode: 21   score: 500.0   memory length: 2000   epsilon: 0.02293966527733732\n",
      "episode: 22   score: 500.0   memory length: 2000   epsilon: 0.013910130026340556\n",
      "episode: 23   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 24   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 25   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 26   score: 432.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 27   score: 396.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 28   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 29   score: 328.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 30   score: 457.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 31   score: 450.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 32   score: 418.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 33   score: 390.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 34   score: 484.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 35   score: 359.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 36   score: 372.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 37   score: 403.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 38   score: 350.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 39   score: 387.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 40   score: 374.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 41   score: 400.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 42   score: 356.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 43   score: 371.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 44   score: 350.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 45   score: 347.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 46   score: 413.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 47   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 48   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 49   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 50   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 51   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 52   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 53   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 54   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 55   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 56   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 57   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 58   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 59   score: 66.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 60   score: 123.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 61   score: 131.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 62   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 63   score: 335.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 64   score: 211.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 65   score: 225.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 66   score: 213.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 67   score: 255.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 68   score: 423.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 69   score: 382.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 70   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 71   score: 386.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 72   score: 430.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 73   score: 342.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 74   score: 289.0   memory length: 2000   epsilon: 0.009998671593271896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 75   score: 309.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 76   score: 269.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 77   score: 272.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 78   score: 243.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 79   score: 244.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 80   score: 282.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 81   score: 302.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 82   score: 278.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 83   score: 268.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 84   score: 236.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 85   score: 226.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 86   score: 252.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 87   score: 236.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 88   score: 256.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 89   score: 231.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 90   score: 251.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 91   score: 241.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 92   score: 256.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 93   score: 253.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 94   score: 252.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 95   score: 250.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 96   score: 310.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 97   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 98   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 99   score: 359.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 100   score: 322.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 101   score: 300.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 102   score: 203.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 103   score: 205.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 104   score: 215.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 105   score: 294.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 106   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 107   score: 327.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 108   score: 168.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 109   score: 82.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 110   score: 188.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 111   score: 266.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 112   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 113   score: 355.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 114   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 115   score: 374.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 116   score: 375.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 117   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 118   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 119   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 120   score: 411.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 121   score: 443.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 122   score: 372.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 123   score: 417.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 124   score: 421.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 125   score: 462.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 126   score: 462.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 127   score: 367.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 128   score: 162.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 129   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 130   score: 364.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 131   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 132   score: 422.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 133   score: 470.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 134   score: 354.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 135   score: 321.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 136   score: 363.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 137   score: 375.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 138   score: 403.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 139   score: 470.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 140   score: 497.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 141   score: 487.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 142   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 143   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 144   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 145   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 146   score: 393.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 147   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 148   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 149   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 150   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 151   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 152   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 153   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 154   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 155   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 156   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 157   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 158   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 159   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 160   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 161   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 162   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 163   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 164   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 165   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 166   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 167   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 168   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 169   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 170   score: 372.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 171   score: 172.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 172   score: 160.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 173   score: 279.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 174   score: 304.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 175   score: 436.0   memory length: 2000   epsilon: 0.009998671593271896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 176   score: 438.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 177   score: 440.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 178   score: 487.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 179   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 180   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 181   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 182   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 183   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 184   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 185   score: 443.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 186   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 187   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 188   score: 489.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 189   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 190   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 191   score: 475.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 192   score: 467.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 193   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 194   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 195   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 196   score: 362.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 197   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 198   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 199   score: 486.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 200   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 201   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 202   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 203   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 204   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 205   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 206   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 207   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 208   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 209   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 210   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 211   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 212   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 213   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 214   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 215   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 216   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 217   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 218   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 219   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 220   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 221   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 222   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 223   score: 363.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 224   score: 340.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 225   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 226   score: 483.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 227   score: 297.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 228   score: 283.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 229   score: 243.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 230   score: 228.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 231   score: 339.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 232   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 233   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 234   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 235   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 236   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 237   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 238   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 239   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 240   score: 195.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 241   score: 223.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 242   score: 337.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 243   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 244   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 245   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 246   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 247   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 248   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 249   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 250   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 251   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 252   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 253   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 254   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 255   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 256   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 257   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 258   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 259   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 260   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 261   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 262   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 263   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 264   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 265   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 266   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 267   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 268   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 269   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 270   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 271   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 272   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 273   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 274   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 275   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 276   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 277   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 278   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 279   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 280   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 281   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 282   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 283   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 284   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 285   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 286   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 287   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 288   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 289   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 290   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 291   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 292   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 293   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 294   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 295   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 296   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 297   score: 8.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 298   score: 178.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 299   score: 18.0   memory length: 2000   epsilon: 0.009998671593271896\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "EPISODES = 300\n",
    "\n",
    "\n",
    "# Double DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and replay memory & target q network\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = True\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the Double DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/cartpole_ddqn.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_next = self.model.predict(update_target)\n",
    "        target_val = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_next[i])\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    target_val[i][a])\n",
    "\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, you can play until 500 time step\n",
    "    env = gym.make('CartPole-v1')\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DoubleDQNAgent(state_size, action_size)\n",
    "    agent.load_model = 1\n",
    "    \n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = reward if not done or score == 499 else -100\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            agent.train_model()\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.update_target_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/cartpole_ddqn.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    agent.model.save_weights(\"./save_model/cartpole_ddqn.h5\")\n",
    "                    break\n",
    "\n",
    "        # save the model\n",
    "        if e % 50 == 0:\n",
    "            agent.model.save_weights(\"./save_model/cartpole_ddqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        agent = DoubleDQNAgent(state_size, action_size)\n",
    "        agent.load_model = 1\n",
    "        agent.epsilon = 0\n",
    "\n",
    "        # Use the Gym Monitor wrapper to evaluate the agent and record video\n",
    "        gym_monitor_path = \"./gym_monitor_output\"\n",
    "        env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)\n",
    "        for _ in range(10):\n",
    "                done = False\n",
    "                obs = env.reset()\n",
    "                obs = np.reshape(obs, [1, state_size])\n",
    "                total_reward = 0.0\n",
    "                while not done:\n",
    "                    action = agent.get_action(obs)\n",
    "                    next_obs, reward, done, info = env.step(action)\n",
    "                    next_obs = np.reshape(next_obs, [1, state_size])\n",
    "                    obs = next_obs\n",
    "                    total_reward += reward\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Другие улучшения DQN\n",
    "\n",
    "## Prioritized Experience Replay\n",
    "\n",
    "Минибатчи из памяти выбираются не с равномерным распределением, а добавляем туда больше примеров, в которых предсказанные значения Q сильнее всего отличаются от корректных. Т.е. примеры с максимальным **TD error** получают максимальный приоритет.\n",
    "\n",
    "## Dueling networks\n",
    "\n",
    "Основная идея в том, что мы разделяем нашу сеть на две головы, одна из которых предсказывает абсолютное значение состояния \\\\( V(S) \\\\), а вторая - относительное преимущество одний действий над другими \\\\( A(s, a) = Q(s, a) - V(s) \\\\). Это преимущество называется advantage. Далее из этих двух значений мы собираем нашу Q-функцию, как \\\\( Q(s,a) = V(s) + A(a) \\\\)\n",
    "\n",
    "## Noisy nets\n",
    "\n",
    "Т.к. по мере обучения агент будет стремиться выбирать состояния с максимальным Q, среду уже исследованных, это может помешать ему найти более эффективные состояния, в которых он ещё не было. Одним из решений этой проблемы является использование детерминированной и случайной нейросети, распределение параметров которой так же обучается с помощью градиентного спуска.\n",
    "\n",
    "## Multi-step learning/n-step learning\n",
    "\n",
    "Основная идея в том, чтобы считать функцию ошибки не по двум соседним примерам, а сразу по n. Это позволяет сети лучше запоминать длинные последовательности действий.\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "Детерминированное значение Q заменяется случайным распределением Z с некоторыми параметрами, которые определяются в ходе обучения.\n",
    "\n",
    "# Rainbow\n",
    "\n",
    "State of the art в развитии Q-обучения - набор перечисленных выше твиков. На графике ниже сравнение различных алгоритмов по количеству очков, усреднённое по играм Atari в сравнении со средними результатами человека.\n",
    "\n",
    "<img src=\"rainbow_dqn.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "\n",
    "Мы используем нейросеть, которая получает на вход вектор состояния среды, а на выход сразу выдаёт вектор необходимых действий. Такой подход называется DPG (Deep Policy Gradients) или, в случае детерминированной стратегии, DDPG (Deep Deterministic Policy Gradients). Для подстройки весов в этом случае мы будем использовать градиент:\n",
    "\\\\[ - \\nabla log P(s, a, \\theta) R \\\\]\n",
    "где P - предсказание вероятности действий нейросетью, а R - полученное вознаграждение.\n",
    "\n",
    "Основной минус этого метода - необходимо дожидаться конца эпизода для получения куммулятивного вознаграждения. В отличии от DQN возможна работа с непрерывными действиями, тогда, как DQN может работать только с дискретным набором.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari] in d:\\anaconda3\\envs\\neew\\lib\\site-packages (0.15.4)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in d:\\anaconda3\\envs\\neew\\lib\\site-packages (from gym[atari]) (1.2.2)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in d:\\anaconda3\\envs\\neew\\lib\\site-packages (from gym[atari]) (1.3.2)\n",
      "Requirement already satisfied: opencv-python in d:\\anaconda3\\envs\\neew\\lib\\site-packages (from gym[atari]) (4.1.2.30)\n",
      "Requirement already satisfied: scipy in d:\\anaconda3\\envs\\neew\\lib\\site-packages (from gym[atari]) (1.3.1)\n",
      "Requirement already satisfied: six in d:\\anaconda3\\envs\\neew\\lib\\site-packages (from gym[atari]) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in d:\\anaconda3\\envs\\neew\\lib\\site-packages (from gym[atari]) (1.17.3)\n",
      "Collecting atari-py~=0.2.0; extra == \"atari\"\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/9e/6d6c4f1f3c2cc72022458197964827b0304e29e4e774320a4fb32eaaf9cd/atari_py-0.2.6-cp36-cp36m-win_amd64.whl (1.8MB)\n",
      "Collecting Pillow; extra == \"atari\"\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/ee/289ddb9884665aba9ad10d88c4edf867b87bcb93e3acbeeac41d30d87865/Pillow-6.2.1-cp36-cp36m-win_amd64.whl (2.0MB)\n",
      "Requirement already satisfied: future in d:\\anaconda3\\envs\\neew\\lib\\site-packages (from pyglet<=1.3.2,>=1.2.0->gym[atari]) (0.18.2)\n",
      "Installing collected packages: atari-py, Pillow\n",
      "Successfully installed Pillow-6.2.1 atari-py-0.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 220)               1100      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 220)               48620     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 2)                 442       \n",
      "=================================================================\n",
      "Total params: 50,162\n",
      "Trainable params: 50,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 27.0\n",
      "episode: 1   score: 10.0\n",
      "episode: 2   score: 13.0\n",
      "episode: 3   score: 11.0\n",
      "episode: 4   score: 8.0\n",
      "episode: 5   score: 23.0\n",
      "episode: 6   score: 18.0\n",
      "episode: 7   score: 27.0\n",
      "episode: 8   score: 36.0\n",
      "episode: 9   score: 43.0\n",
      "episode: 10   score: 33.0\n",
      "episode: 11   score: 34.0\n",
      "episode: 12   score: 32.0\n",
      "episode: 13   score: 19.0\n",
      "episode: 14   score: 20.0\n",
      "episode: 15   score: 14.0\n",
      "episode: 16   score: 9.0\n",
      "episode: 17   score: 12.0\n",
      "episode: 18   score: 10.0\n",
      "episode: 19   score: 11.0\n",
      "episode: 20   score: 9.0\n",
      "episode: 21   score: 18.0\n",
      "episode: 22   score: 20.0\n",
      "episode: 23   score: 14.0\n",
      "episode: 24   score: 15.0\n",
      "episode: 25   score: 14.0\n",
      "episode: 26   score: 15.0\n",
      "episode: 27   score: 15.0\n",
      "episode: 28   score: 14.0\n",
      "episode: 29   score: 16.0\n",
      "episode: 30   score: 20.0\n",
      "episode: 31   score: 16.0\n",
      "episode: 32   score: 20.0\n",
      "episode: 33   score: 12.0\n",
      "episode: 34   score: 14.0\n",
      "episode: 35   score: 13.0\n",
      "episode: 36   score: 11.0\n",
      "episode: 37   score: 13.0\n",
      "episode: 38   score: 10.0\n",
      "episode: 39   score: 8.0\n",
      "episode: 40   score: 13.0\n",
      "episode: 41   score: 11.0\n",
      "episode: 42   score: 9.0\n",
      "episode: 43   score: 11.0\n",
      "episode: 44   score: 14.0\n",
      "episode: 45   score: 13.0\n",
      "episode: 46   score: 17.0\n",
      "episode: 47   score: 17.0\n",
      "episode: 48   score: 16.0\n",
      "episode: 49   score: 21.0\n",
      "episode: 50   score: 19.0\n",
      "episode: 51   score: 30.0\n",
      "episode: 52   score: 28.0\n",
      "episode: 53   score: 41.0\n",
      "episode: 54   score: 28.0\n",
      "episode: 55   score: 55.0\n",
      "episode: 56   score: 25.0\n",
      "episode: 57   score: 30.0\n",
      "episode: 58   score: 29.0\n",
      "episode: 59   score: 37.0\n",
      "episode: 60   score: 58.0\n",
      "episode: 61   score: 47.0\n",
      "episode: 62   score: 32.0\n",
      "episode: 63   score: 45.0\n",
      "episode: 64   score: 24.0\n",
      "episode: 65   score: 25.0\n",
      "episode: 66   score: 10.0\n",
      "episode: 67   score: 13.0\n",
      "episode: 68   score: 21.0\n",
      "episode: 69   score: 37.0\n",
      "episode: 70   score: 18.0\n",
      "episode: 71   score: 38.0\n",
      "episode: 72   score: 27.0\n",
      "episode: 73   score: 39.0\n",
      "episode: 74   score: 35.0\n",
      "episode: 75   score: 52.0\n",
      "episode: 76   score: 32.0\n",
      "episode: 77   score: 26.0\n",
      "episode: 78   score: 45.0\n",
      "episode: 79   score: 40.0\n",
      "episode: 80   score: 49.0\n",
      "episode: 81   score: 70.0\n",
      "episode: 82   score: 27.0\n",
      "episode: 83   score: 27.0\n",
      "episode: 84   score: 114.0\n",
      "episode: 85   score: 52.0\n",
      "episode: 86   score: 35.0\n",
      "episode: 87   score: 30.0\n",
      "episode: 88   score: 53.0\n",
      "episode: 89   score: 29.0\n",
      "episode: 90   score: 24.0\n",
      "episode: 91   score: 33.0\n",
      "episode: 92   score: 26.0\n",
      "episode: 93   score: 37.0\n",
      "episode: 94   score: 33.0\n",
      "episode: 95   score: 20.0\n",
      "episode: 96   score: 40.0\n",
      "episode: 97   score: 29.0\n",
      "episode: 98   score: 23.0\n",
      "episode: 99   score: 30.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EPISODES = 100\n",
    "\n",
    "\n",
    "# This is Policy Gradient agent for the Cartpole\n",
    "# In this example, we use DPG algorithm which uses monte-carlo update rule\n",
    "class DPGAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.01\n",
    "        self.hidden1, self.hidden2 = 220, 220\n",
    "\n",
    "        # create model for policy network\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        # lists for the states, actions and rewards\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/cartpole_dpg.h5\")\n",
    "\n",
    "    # approximate policy using Neural Network\n",
    "    # state is input and probability of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hidden1, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "        model.add(Dense(self.hidden2, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='softmax', kernel_initializer='glorot_uniform'))\n",
    "        model.summary()\n",
    "        # Using categorical crossentropy as a loss is a trick to easily\n",
    "        # implement the policy gradient. Categorical cross entropy is defined\n",
    "        # H(p, q) = sum(p_i * log(q_i)). For the action taken, a, you set \n",
    "        # p_a = advantage. q_a is the output of the policy network, which is\n",
    "        # the probability of taking the action a, i.e. policy(s, a). \n",
    "        # All other p_i are zero, thus we have H(p, q) = A * log(policy(s, a))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.model.predict(state, batch_size=1).flatten()\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # In Policy Gradient, Q function is not available.\n",
    "    # Instead agent uses sample returns for evaluating policy\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    # save <s, a ,r> of each step\n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self):\n",
    "        episode_length = len(self.states)\n",
    "\n",
    "        discounted_rewards = self.discount_rewards(self.rewards)\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "\n",
    "        update_inputs = np.zeros((episode_length, self.state_size))\n",
    "        advantages = np.zeros((episode_length, self.action_size))\n",
    "\n",
    "        for i in range(episode_length):\n",
    "            update_inputs[i] = self.states[i]\n",
    "            advantages[i][self.actions[i]] = discounted_rewards[i]\n",
    "\n",
    "        self.model.fit(update_inputs, advantages, epochs=1, verbose=0)\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, you can play until 500 time step\n",
    "    env = gym.make('CartPole-v1')\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # make DPG agent\n",
    "    agent = DPGAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            reward = reward if not done or score == 499 else -100\n",
    "\n",
    "            # save the sample <s, a, r> to the memory\n",
    "            agent.append_sample(state, action, reward)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode, agent learns from sample returns\n",
    "                agent.train_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/cartpole_dpg.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    agent.model.save_weights(\"./save_model/cartpole_dpg.h5\")\n",
    "                    break\n",
    "\n",
    "        # save the model\n",
    "        if e % 50 == 0:\n",
    "            agent.model.save_weights(\"./save_model/cartpole_dpg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_53 (Dense)             (None, 220)               1100      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 220)               48620     \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 2)                 442       \n",
      "=================================================================\n",
      "Total params: 50,162\n",
      "Trainable params: 50,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-22371e98b52d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         )\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mrender_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ansi'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mansi_mode\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     raise ImportError('''\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\pyglet\\gl\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcarbon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCarbonConfig\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;31m# XXX remove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        agent = DPGAgent(state_size, action_size)\n",
    "        agent.load_model = 1\n",
    "        agent.epsilon = 0\n",
    "\n",
    "        # Use the Gym Monitor wrapper to evaluate the agent and record video\n",
    "        gym_monitor_path = \"./gym_monitor_output\"\n",
    "        env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)\n",
    "        for _ in range(10):\n",
    "                done = False\n",
    "                obs = env.reset()\n",
    "                obs = np.reshape(obs, [1, state_size])\n",
    "                total_reward = 0.0\n",
    "                while not done:\n",
    "                    action = agent.get_action(obs)\n",
    "                    next_obs, reward, done, info = env.step(action)\n",
    "                    next_obs = np.reshape(next_obs, [1, state_size])\n",
    "                    obs = next_obs\n",
    "                    total_reward += reward\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшения метода Policy Gradients\n",
    "\n",
    "Такого большого набора твиков, как для DQN для DPG не наблюдается. Наиболее известны методы: TRPO (Trust Region Policy Optimization) и PPO (Proximal Policy Optimization). В них заложена несколько разная математика, но суть обоих методов в ограничении изменения весов за один прогон для того, чтобы выбросы не портили выученную стратегию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Комбинирование подходов\n",
    "\n",
    "## Actor-critic\n",
    "\n",
    "Можно взять две сети, одна из которых будет предсказывать действия, а вторая - оценивать, насколько эти действия хороши, т.е. выдавать значение Q. Помимо самих действий ей на вход мы так же подадим состояне.\n",
    "\n",
    "<img src=\"actor_critic.png\">\n",
    "\n",
    "Плюс в том, что нам не обязательно дожидаться окончания эпизода для обучения.\n",
    "\n",
    "## Advantage-Actor-Critic, A2C\n",
    "\n",
    "Вместо вычисления градиентов от абсолютного значения \\\\( Q(s, a) \\\\) мы можем использовать относительное преимущество одний действия над другими \\\\( A(s, a) = Q(s, a) - V(s) \\)). Тогда если \\\\( A(s, a) > 0 \\\\), то градиентный спуск будет изменять все веса, повышая вероятность предсказанных действий. Если же \\\\( A(s, a) < 0 \\\\), то градиентный спуск будет понижать вероятность таких действий. \\\\( V(s) \\\\) при этом показывает, насколько состояние хорошо само по себе: если мы в двух шагах от вершины Эвереста, то это очень хороший state, а если мы летим в пропасть, то state крайне фиговый, чтобы мы в нём не делали (если, конечно, у нас нет с собой парашюта).\n",
    "\n",
    "При использовании такого подхода функцию Q(s, a) можно заменить прямо на полученное вознаграждение за некоторое действие вознаграждение r. Тогда \\\\( A(s,a) = r - V(s) \\\\). При этом получается, что сети Actor и Critic можно объединить в одну с двумя головами, что улучшает переиспользование весов и ускоряет обучение.\n",
    "\n",
    "<img src=\"a2c.png\" width=700>\n",
    "\n",
    "A3C или Asynchronous Advantage Actor Critic - означает, что у нас есть сервер, собирающий результаты с нескольких Actor'ов и обновляющий веса, когда наберётся батч достаточного размера. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_56 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 170\n",
      "Trainable params: 170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_58 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 145\n",
      "Trainable params: 145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 10.0\n",
      "episode: 1   score: 11.0\n",
      "episode: 2   score: 11.0\n",
      "episode: 3   score: 11.0\n",
      "episode: 4   score: 12.0\n",
      "episode: 5   score: 10.0\n",
      "episode: 6   score: 13.0\n",
      "episode: 7   score: 19.0\n",
      "episode: 8   score: 12.0\n",
      "episode: 9   score: 8.0\n",
      "episode: 10   score: 8.0\n",
      "episode: 11   score: 12.0\n",
      "episode: 12   score: 10.0\n",
      "episode: 13   score: 9.0\n",
      "episode: 14   score: 9.0\n",
      "episode: 15   score: 14.0\n",
      "episode: 16   score: 10.0\n",
      "episode: 17   score: 10.0\n",
      "episode: 18   score: 10.0\n",
      "episode: 19   score: 12.0\n",
      "episode: 20   score: 8.0\n",
      "episode: 21   score: 7.0\n",
      "episode: 22   score: 9.0\n",
      "episode: 23   score: 8.0\n",
      "episode: 24   score: 10.0\n",
      "episode: 25   score: 12.0\n",
      "episode: 26   score: 8.0\n",
      "episode: 27   score: 12.0\n",
      "episode: 28   score: 9.0\n",
      "episode: 29   score: 10.0\n",
      "episode: 30   score: 11.0\n",
      "episode: 31   score: 12.0\n",
      "episode: 32   score: 11.0\n",
      "episode: 33   score: 13.0\n",
      "episode: 34   score: 27.0\n",
      "episode: 35   score: 15.0\n",
      "episode: 36   score: 10.0\n",
      "episode: 37   score: 15.0\n",
      "episode: 38   score: 17.0\n",
      "episode: 39   score: 14.0\n",
      "episode: 40   score: 15.0\n",
      "episode: 41   score: 10.0\n",
      "episode: 42   score: 13.0\n",
      "episode: 43   score: 9.0\n",
      "episode: 44   score: 9.0\n",
      "episode: 45   score: 19.0\n",
      "episode: 46   score: 22.0\n",
      "episode: 47   score: 12.0\n",
      "episode: 48   score: 12.0\n",
      "episode: 49   score: 9.0\n",
      "episode: 50   score: 15.0\n",
      "episode: 51   score: 8.0\n",
      "episode: 52   score: 10.0\n",
      "episode: 53   score: 9.0\n",
      "episode: 54   score: 11.0\n",
      "episode: 55   score: 10.0\n",
      "episode: 56   score: 11.0\n",
      "episode: 57   score: 24.0\n",
      "episode: 58   score: 13.0\n",
      "episode: 59   score: 22.0\n",
      "episode: 60   score: 12.0\n",
      "episode: 61   score: 16.0\n",
      "episode: 62   score: 18.0\n",
      "episode: 63   score: 18.0\n",
      "episode: 64   score: 8.0\n",
      "episode: 65   score: 11.0\n",
      "episode: 66   score: 8.0\n",
      "episode: 67   score: 23.0\n",
      "episode: 68   score: 7.0\n",
      "episode: 69   score: 11.0\n",
      "episode: 70   score: 11.0\n",
      "episode: 71   score: 10.0\n",
      "episode: 72   score: 10.0\n",
      "episode: 73   score: 10.0\n",
      "episode: 74   score: 14.0\n",
      "episode: 75   score: 13.0\n",
      "episode: 76   score: 15.0\n",
      "episode: 77   score: 11.0\n",
      "episode: 78   score: 11.0\n",
      "episode: 79   score: 18.0\n",
      "episode: 80   score: 14.0\n",
      "episode: 81   score: 17.0\n",
      "episode: 82   score: 10.0\n",
      "episode: 83   score: 12.0\n",
      "episode: 84   score: 9.0\n",
      "episode: 85   score: 11.0\n",
      "episode: 86   score: 11.0\n",
      "episode: 87   score: 11.0\n",
      "episode: 88   score: 18.0\n",
      "episode: 89   score: 8.0\n",
      "episode: 90   score: 9.0\n",
      "episode: 91   score: 58.0\n",
      "episode: 92   score: 8.0\n",
      "episode: 93   score: 8.0\n",
      "episode: 94   score: 8.0\n",
      "episode: 95   score: 18.0\n",
      "episode: 96   score: 24.0\n",
      "episode: 97   score: 12.0\n",
      "episode: 98   score: 9.0\n",
      "episode: 99   score: 21.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EPISODES = 100\n",
    "\n",
    "\n",
    "# A2C(Advantage Actor-Critic) agent for the Cartpole\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.001\n",
    "        self.critic_lr = 0.005\n",
    "\n",
    "        # create model for policy network\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.actor.load_weights(\"./save_model/cartpole_actor.h5\")\n",
    "            self.critic.load_weights(\"./save_model/cartpole_critic.h5\")\n",
    "\n",
    "    # approximate policy and value using Neural Network\n",
    "    # actor: state is input and probability of each action is output of model\n",
    "    def build_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.summary()\n",
    "        # See note regarding crossentropy in cartpole_reinforce.py\n",
    "        actor.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(lr=self.actor_lr))\n",
    "        return actor\n",
    "\n",
    "    # critic: state is input and value of state is output of model\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.add(Dense(self.value_size, activation='linear',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.summary()\n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
    "        return critic\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        target = np.zeros((1, self.value_size))\n",
    "        advantages = np.zeros((1, self.action_size))\n",
    "\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.discount_factor * (next_value) - value\n",
    "            target[0][0] = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
    "        self.critic.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, maximum length of episode is 500\n",
    "    env = gym.make('CartPole-v1')\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # make A2C agent\n",
    "    agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = reward if not done or score == 499 else -100\n",
    "\n",
    "            agent.train_model(state, action, reward, next_state, done)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500.0 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/cartpole_a2c.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    break\n",
    "\n",
    "        # save the model\n",
    "        if e % 50 == 0:\n",
    "            agent.actor.save_weights(\"./save_model/cartpole_actor.h5\")\n",
    "            agent.critic.save_weights(\"./save_model/cartpole_critic.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 170\n",
      "Trainable params: 170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_62 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 145\n",
      "Trainable params: 145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-014f247bbbb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         )\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mrender_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ansi'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mansi_mode\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     raise ImportError('''\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\Neew\\lib\\site-packages\\pyglet\\gl\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'darwin'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[1;31m# noqa: F821\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;31m# XXX remove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        agent = A2CAgent(state_size, action_size)\n",
    "        agent.load_model = 1\n",
    "        agent.epsilon = 0\n",
    "\n",
    "        # Use the Gym Monitor wrapper to evaluate the agent and record video\n",
    "        gym_monitor_path = \"./gym_monitor_output\"\n",
    "        env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)\n",
    "        for _ in range(10):\n",
    "                done = False\n",
    "                obs = env.reset()\n",
    "                obs = np.reshape(obs, [1, state_size])\n",
    "                total_reward = 0.0\n",
    "                while not done:\n",
    "                    action = agent.get_action(obs)\n",
    "                    next_obs, reward, done, info = env.step(action)\n",
    "                    next_obs = np.reshape(next_obs, [1, state_size])\n",
    "                    obs = next_obs\n",
    "                    total_reward += reward\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based\n",
    "\n",
    "Ещё один подход заключается в том, что мы можем обучать нейросеть предсказывать следующее состояние среды, подавая ей на вход действия и предыдущее состояние. Таким образом нейросеть учит поведение среды. Для того, чтобы выбрать оптимальные действия, нам придётся прогнать все возможные действия через предсказание нейросети, поэтому такой подход применим только при малой размерности пространства действий.\n",
    "\n",
    "# Imitation learning\n",
    "\n",
    "Для того, чтобы агент выучил сложную последовательность действий, можно искуственно поставить его в конец этой траектории, тогда он быстро выучит, как пройти небольшой участок. После этого его можно поставить чуть дальше и так, пока агент на научится выполнять всю последовательность. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
